{"cells":[{"metadata":{},"cell_type":"raw","source":"Yosyp Skrobynets CIS 3920 Fall 2020"},{"metadata":{},"cell_type":"markdown","source":"\n\n------\n\n\n## ü•ë About The Project\n\n**Introduction**\n\nWhat is being tried to accomplish from this project?\nEducational exploration of machine learning models applied to a dataset of female breast cancer observations.\n\nProject Description/Abstract\nThis Work is dedicated to my aunt Larisa , that i hope gets well.\nThe goal of this project is to pull a variety of cancer dtasets available online into single analysis that will contain both visualisation and  perdiction aspects.\nWisconsin Breast Cancer Dataset is best suited for the machine laerning exploration as it will require dimentionality reduction , hence making it more complex.\nObservations in this dataset are measurements of images of Female breast cancer tumor cells.\n\nDataset being used and descriptions of the dataset:\n* Attribute Information:\n*  ID number\n*  Diagnosis (M = malignant, B = benign)\n* Ten real-valued features are computed for each cell nucleus:\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 / area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\n**Missing attribute values: none**\n\n**Class distribution: 357 benign, 212 malignant**\nThis Dataset can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n## Project methodology \n\n- [ü•ù  Import The Data](#1.0 )\n\n- [üçç Explore The Data](#1.2)\n\n- [üçÑ  PreProcess The Data](#1.3)\n\n- [üçí Model & Validate The Data](#2.0)\n - **[Principal Component Analysis](#PCA)\n - **[Multinominal Logistic Regression](#GLM)\n - **[Support Vector Machines](#SVM)\n - **[K Nearest Neighbour](#KNN)\n - **[C 5.0](#C5)\n - **[Cost Sensitive C 5.0 ](#csC5)\n     \n- [ü•í Conclude The Data](#3.0)\n"},{"metadata":{},"cell_type":"markdown","source":" #1.0\n ## Import  The Data  ü•ù\n "},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(warn=-1)\nwarning=FALSE\nlibrary(\"ggplot2\")\nlibrary(\"GGally\")\nlibrary(\"caret\")\nlibrary(\"tidyverse\")\nlibrary(\"viridisLite\")\nlibrary(\"funModeling\")\n\nlibrary(\"gridExtra\")\n\n\ndata.cancer1 = read.csv(\"../input/cancer69/data.csv\")\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#1.2\n\n## üçç   Explore The Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"str(data.cancer1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.cancer1$X <- NULL\ndata.cancer1$id <- NULL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width=13, repr.plot.height = 10)\nplot_num(log(data.cancer1[,-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(repr) ;\n\n\n\n\n\ncancer=data.cancer1[c(-1,-25)]\nggcorr(cancer,label=T)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(dplyr)\nfreq(data.cancer1$diagnosis)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#1.3\n##  üçÑ  PreProcess The Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(111)\n index= sample(2, nrow(data.cancer1),\n              replace = TRUE,\n              prob = c(0.8, 0.2))\ntrain = data.cancer1[index==1,]\ntest = data.cancer1[index==2,]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#PCA-Principal Component Analysis \n \n PCA is a linear dimentionality reduction function that combines miltiple variables into one that is projected  on a single dimention.\n It is used to help the user choose which data combination perfoms better explaining the variance of given dataset.\n In our case PC1 , the first component, explains 44% of variability in the given dataset using 31 predictors.\n Thses componets are later used as predictors for the machine learning algorithms.\n \n Depending on users preference ,one can choose as much as all componets, or as little as one .\n We shall choose 2 to begin with , PC1 amd PC2."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca <- prcomp(train[,-1],\n             center = TRUE,\n             scale. = TRUE)\n           \nsummary(pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the Variable Contribution Chart which represents which particular column is more dominant in explaining the variance.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"library(\"gridExtra\")\nlibrary(\"FactoMineR\")\nlibrary(\"factoextra\")\noptions(repr.plot.width=16, repr.plot.height = 8)\np1 <- fviz_contrib(pca, choice=\"var\", axes=1, fill=\"pink\", color=\"black\", top=5)\np2 <- fviz_contrib(pca, choice=\"var\", axes=2, fill=\"skyblue\", color=\"black\", top=5)\np3 <- fviz_contrib(pca, choice=\"var\", axes=3, fill=\"skyblue\", color=\"black\", top=5)\np4 <- fviz_contrib(pca, choice=\"var\", axes=4, fill=\"skyblue\", color=\"black\", top=5)\np5 <- fviz_contrib(pca, choice=\"var\", axes=5, fill=\"skyblue\", color=\"black\", top=5)\ngrid.arrange(p1,p2,p3,p4,p5,ncol=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are sourcing the function directly from package without loading it into the kernel.(i could not install the package)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"source('https://raw.githubusercontent.com/vqv/ggbiplot/master/R/ggbiplot.r')\n\nsource('https://raw.githubusercontent.com/vqv/ggbiplot/master/R/ggscreeplot.r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a visual representation if a PCA.\nThe first chart explains the cumulative persent contibution of PCA.\nSecond is the PCA plot itself where the componets are clearly separable."},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width=16, repr.plot.height = 10)\n\ng <- ggbiplot(pca,\n              obs.scale = 1,\n              var.scale = 1,\n              groups = train$diagnosis,\n              ellipse = TRUE,\n              circle = TRUE,\n              ellipse.prob = 0.68)\ng <- g + scale_color_discrete(name = '')\ng <- g + theme(legend.direction = 'horizontal',\n               legend.position = 'top')\npca.ex=ggscreeplot(pca)+ geom_vline(xintercept = 11:15, colour=\"purple\", linetype = \"longdash\")\ngrid.arrange(pca.ex,g,ncol=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we re spliting the dataset to prepare for modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.pca <- predict(pca, train)\ntrain.pca <- data.frame(train.pca, train[1])\ntest.pca <- predict(pca, test)\ntest.pca <- data.frame(test.pca, test[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#2.0\n## üçí Model & Validate The Data"},{"metadata":{},"cell_type":"markdown","source":"MODELS\n"},{"metadata":{},"cell_type":"markdown","source":"Simple machine learning tools are used from classification category."},{"metadata":{},"cell_type":"markdown","source":"#GLM"},{"metadata":{},"cell_type":"markdown","source":"The first algo is a The Holy Logistic Regression.\nPredicting diagnosis using PC1 and PC2, model accuracy is 95% Test and 94%Train ;Thank You , Glm!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(nnet)\n\nGLM = multinom(diagnosis~PC1+PC2, data = train.pca)\nglm.prediction.train = predict(GLM, train.pca)\n\n\ncomatrix.glm.train = confusionMatrix(glm.prediction.train, train.pca$diagnosis)\ncomatrix.glm.train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glm.prediction.test= predict(GLM, test.pca)\n\n\n\ncomatrix.glm.test = confusionMatrix(glm.prediction.test, test.pca$diagnosis)\ncomatrix.glm.test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is used for cross validation in some models"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(caret)\ncontrol <- trainControl(method='repeatedcv', \n                        number=13, \n                        repeats=3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#KNN"},{"metadata":{},"cell_type":"markdown","source":"KNN:K-Nearest Neigbour \nKNN is a standart classification function that uses euclidian distance  to calculate desision boundaies depending on number of observations K.\nUsually , the knn curve has a low accuracy for low number of neighbours , peaking at 5-10 ."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn10<- train(diagnosis ~ PC1+PC2, data = train.pca, method = \"knn\", trControl = control,tuneLength = 4)\nknn10.1=plot(knn10)\nknn100<- train(diagnosis ~ PC1+PC2, data = train.pca, method = \"knn\", trControl = control,tuneLength = 20)\nknn100.1=plot(knn100)\nknn1000<- train(diagnosis ~ PC1+PC2, data = train.pca, method = \"knn\", trControl = control,tuneLength = 60)\nknn1000.1=plot(knn1000)\ngrid.arrange(knn10.1,knn100.1,knn1000.1,ncol=3)\noptions(repr.plot.width=30, repr.plot.height = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#2"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.prediction.train= predict(knn10, train.pca)\ncomatrix.knn.train = confusionMatrix(knn.prediction.train, train.pca$diagnosis, positive = \"M\")\ncomatrix.knn.train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glm.prediction.test= predict(knn10, test.pca)\ncomatrix.knn.test = confusionMatrix(glm.prediction.test, test.pca$diagnosis, positive = \"M\")\ncomatrix.knn.test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width=10, repr.plot.height = 10)\nlibrary(e1071)\n\nSVM <- svm(diagnosis ~ PC1+PC2, data = train.pca, kernel = \"linear\", cost = 13)\nsvm.1=plot(SVM,train.pca,PC1~PC2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.prediction.train<- predict(SVM, train.pca)\ncomatrix.svm.train <- confusionMatrix(svm.prediction.train, train.pca$diagnosis, positive = \"M\")\ncomatrix.svm.train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.prediction.test<- predict(SVM, test.pca)\ncomatrix.svm.test <- confusionMatrix(svm.prediction.test, test.pca$diagnosis, positive = \"M\")\ncomatrix.svm.test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM.r <- svm(diagnosis ~ PC1+PC2, data = train.pca, kernel = \"radial\", cost = 40)\nsvm.2=plot(SVM.r,train.pca,PC1~PC2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.prediction.train<- predict(SVM.r, train.pca)\ncomatrix.svm.train <- confusionMatrix(svm.prediction.train, train.pca$diagnosis, positive = \"M\")\ncomatrix.svm.train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.prediction.test<- predict(SVM.r, test.pca)\ncomatrix.svm.test <- confusionMatrix(svm.prediction.test, test.pca$diagnosis, positive = \"M\")\ncomatrix.svm.test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#C5"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(C50)\nC50 <- C5.0(train.pca,train.pca$diagnosis)\n\nc50.prediction.train <- predict(C50, train.pca)\nc50.comatrix.train <- confusionMatrix(c50.prediction.train, train.pca$diagnosis)\nc50.comatrix.train\n\nc50.prediction.test <- predict(C50, test.pca)\nc50.comatrix.test<- confusionMatrix(c50.prediction.test, test.pca$diagnosis)\nc50.comatrix.test\nplot(C50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#csC5"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width=16, repr.plot.height = 10)\nmod2 <- train(diagnosis ~ PC1+PC2, data = train.pca, \n              method = \"C5.0Cost\",\n              tuneGrid = expand.grid(model = \"tree\", winnow = FALSE,\n                                     trials = c(1:10, (1:5)*10),\n                                     cost = 1:10),\n              trControl = control)\n\nplot(mod2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c50c.predict.train <- predict(mod2, train.pca)\nc50c.comatrix.train <- confusionMatrix(c50c.predict.train, train.pca$diagnosis)\nc50c.comatrix.train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c50c.predict.test <- predict(mod2, test.pca)\nc50c.comatrix.test  <- confusionMatrix(c50c.predict.test, test.pca$diagnosis)\nc50c.comatrix.test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#3.0\n## ü•í Conclude The Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"options(repr.plot.width=26, repr.plot.height = 15)\ncol <- c(\"#06908F\", \"#dbb68f\")\npar(mfrow=c(2,6))\nfourfoldplot(comatrix.glm.train$table, color = col, conf.level = 0, margin = 1, main=paste(\"Logistic.train (\",round(comatrix.glm.train$overall[1]*100),\"%)\",sep=\"\"))\nfourfoldplot(comatrix.glm.test$table, color = col, conf.level = 0, margin = 1, main=paste(\"Logistic.test (\",round(comatrix.glm.test$overall[1]*100),\"%)\",sep=\"\"))\n\nfourfoldplot(c50.comatrix.train$table, color = col, conf.level = 0, margin = 1, main=paste(\"C5.0.train (\",round(c50.comatrix.train$overall[1]*100),\"%)\",sep=\"\"))\nfourfoldplot(c50.comatrix.test$table, color = col, conf.level = 0, margin = 1, main=paste(\"C5.0.test (\",round(c50.comatrix.test$overall[1]*100),\"%)\",sep=\"\"))\n\nfourfoldplot(c50c.comatrix.train$table, color = col, conf.level = 0, margin = 1, main=paste(\"Cost Sensitive C5.0.train (\",round(c50c.comatrix.train$overall[1]*100),\"%)\",sep=\"\"))\nfourfoldplot(c50c.comatrix.test$table, color = col, conf.level = 0, margin = 1, main=paste(\"Cost Sensitive C5.0.test (\",round(c50c.comatrix.test$overall[1]*100),\"%)\",sep=\"\"))\n\nfourfoldplot(comatrix.svm.train$table, color = col, conf.level = 0, margin = 1, main=paste(\"SVM.train (\",round(comatrix.svm.train$overall[1]*100),\"%)\",sep=\"\"))\nfourfoldplot(comatrix.svm.test$table, color = col, conf.level = 0, margin = 1, main=paste(\"SVM.test (\",round(comatrix.svm.test$overall[1]*100),\"%)\",sep=\"\"))\n\nfourfoldplot(comatrix.knn.train$table, color = col, conf.level = 0, margin = 1, main=paste(\"KNN.train(\",round(comatrix.knn.train$overall[1]*100),\"%)\",sep=\"\"))\nfourfoldplot(comatrix.knn.test$table, color = col, conf.level = 0, margin = 1, main=paste(\"KNN.test(\",round(comatrix.knn.test$overall[1]*100),\"%)\",sep=\"\"))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}